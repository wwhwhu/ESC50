E:\LenovoSoftstore\Install\anaconda3\envs\YAMNET\python.exe D:/PycharmProgram/chuanyin/ESC-50/trans_CRNN.py
2023-08-26 13:40:25.560486: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-26 13:40:25.876746: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5451 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 conv1 (Conv2D)              (None, 126, 1249, 30)     300

 batch_normalization (BatchN  (None, 126, 1249, 30)    120
 ormalization)

 pool1 (MaxPooling2D)        (None, 42, 624, 30)       0

 dropout1 (Dropout)          (None, 42, 624, 30)       0

 conv2 (Conv2D)              (None, 40, 622, 60)       16260

 batch_normalization_1 (Batc  (None, 40, 622, 60)      240
 hNormalization)

 pool2 (MaxPooling2D)        (None, 20, 311, 60)       0

 dropout2 (Dropout)          (None, 20, 311, 60)       0

 conv3 (Conv2D)              (None, 18, 309, 60)       32460

 batch_normalization_2 (Batc  (None, 18, 309, 60)      240
 hNormalization)

 pool3 (MaxPooling2D)        (None, 6, 154, 60)        0

 dropout3 (Dropout)          (None, 6, 154, 60)        0

 conv4 (Conv2D)              (None, 4, 152, 60)        32460

 batch_normalization_3 (Batc  (None, 4, 152, 60)       240
 hNormalization)

 pool4 (MaxPooling2D)        (None, 1, 50, 60)         0

 dropout4 (Dropout)          (None, 1, 50, 60)         0

 permute (Permute)           (None, 60, 1, 50)         0

 reshape (Reshape)           (None, 50, 60)            0

 LSTM1 (LSTM)                (None, 50, 60)            29040

 LSTM2 (LSTM)                (None, 60)                29040

 dropout5 (Dropout)          (None, 60)                0

 Linear (Dense)              (None, 50)                3050

 activation (Activation)     (None, 50)                0

=================================================================
Total params: 143,450
Trainable params: 143,030
Non-trainable params: 420
_________________________________________________________________
(1600, 128, 1251, 1) (1600, 50)
2023-08-26 13:40:28.969988: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.
2023-08-26 13:40:34.830302: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:363] Ignored output_format.
2023-08-26 13:40:34.830435: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:366] Ignored drop_control_dependency.
2023-08-26 13:40:34.831003: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: C:\Users\34405\AppData\Local\Temp\tmptayy4z3k
2023-08-26 13:40:34.842914: I tensorflow/cc/saved_model/reader.cc:107] Reading meta graph with tags { serve }
2023-08-26 13:40:34.842979: I tensorflow/cc/saved_model/reader.cc:148] Reading SavedModel debug info (if present) from: C:\Users\34405\AppData\Local\Temp\tmptayy4z3k
2023-08-26 13:40:34.930135: I tensorflow/cc/saved_model/loader.cc:210] Restoring SavedModel bundle.
2023-08-26 13:40:35.074096: I tensorflow/cc/saved_model/loader.cc:194] Running initialization op on SavedModel bundle at path: C:\Users\34405\AppData\Local\Temp\tmptayy4z3k
2023-08-26 13:40:35.156570: I tensorflow/cc/saved_model/loader.cc:283] SavedModel load for tags { serve }; Status: success: OK. Took 325560 microseconds.
2023-08-26 13:40:35.330995: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2023-08-26 13:40:35.463934: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1891] TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following Select TFop(s):
Flex ops: FlexTensorListFromTensor, FlexTensorListGetItem, FlexTensorListReserve, FlexTensorListSetItem, FlexTensorListStack
Details:
	tf.TensorListFromTensor(tensor<50x?x60xf32>, tensor<2xi32>) -> (tensor<!tf_type.variant<tensor<?x60xf32>>>) : {device = ""}
	tf.TensorListFromTensor(tensor<?x?x60xf32>, tensor<2xi32>) -> (tensor<!tf_type.variant<tensor<?x60xf32>>>) : {device = ""}
	tf.TensorListGetItem(tensor<!tf_type.variant<tensor<?x60xf32>>>, tensor<i32>, tensor<2xi32>) -> (tensor<?x60xf32>) : {device = ""}
	tf.TensorListReserve(tensor<2xi32>, tensor<i32>) -> (tensor<!tf_type.variant<tensor<?x60xf32>>>) : {device = ""}
	tf.TensorListSetItem(tensor<!tf_type.variant<tensor<?x60xf32>>>, tensor<i32>, tensor<?x60xf32>) -> (tensor<!tf_type.variant<tensor<?x60xf32>>>) : {device = ""}
	tf.TensorListStack(tensor<!tf_type.variant<tensor<?x60xf32>>>, tensor<2xi32>) -> (tensor<?x?x60xf32>) : {device = "", num_elements = -1 : i64}
See instructions: https://www.tensorflow.org/lite/guide/ops_select
(400, 128, 1251, 1) (400, 50)
train_data.shape (1600, 128, 1251, 1)
train_label.shape (1600, 50)
end_step:  2000
find prune layer: conv1
find prune layer: batch_normalization
find prune layer: conv2
find prune layer: batch_normalization_1
find prune layer: conv3
find prune layer: batch_normalization_2
find prune layer: conv4
find prune layer: batch_normalization_3
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 prune_low_magnitude_conv1 (  (None, 126, 1249, 30)    572
 PruneLowMagnitude)

 prune_low_magnitude_batch_n  (None, 126, 1249, 30)    121
 ormalization (PruneLowMagni
 tude)

 pool1 (MaxPooling2D)        (None, 42, 624, 30)       0

 dropout1 (Dropout)          (None, 42, 624, 30)       0

 prune_low_magnitude_conv2 (  (None, 40, 622, 60)      32462
 PruneLowMagnitude)

 prune_low_magnitude_batch_n  (None, 40, 622, 60)      241
 ormalization_1 (PruneLowMag
 nitude)

 pool2 (MaxPooling2D)        (None, 20, 311, 60)       0

 dropout2 (Dropout)          (None, 20, 311, 60)       0

 prune_low_magnitude_conv3 (  (None, 18, 309, 60)      64862
 PruneLowMagnitude)

 prune_low_magnitude_batch_n  (None, 18, 309, 60)      241
 ormalization_2 (PruneLowMag
 nitude)

 pool3 (MaxPooling2D)        (None, 6, 154, 60)        0

 dropout3 (Dropout)          (None, 6, 154, 60)        0

 prune_low_magnitude_conv4 (  (None, 4, 152, 60)       64862
 PruneLowMagnitude)

 prune_low_magnitude_batch_n  (None, 4, 152, 60)       241
 ormalization_3 (PruneLowMag
 nitude)

 pool4 (MaxPooling2D)        (None, 1, 50, 60)         0

 dropout4 (Dropout)          (None, 1, 50, 60)         0

 permute (Permute)           (None, 60, 1, 50)         0

 reshape (Reshape)           (None, 50, 60)            0

 LSTM1 (LSTM)                (None, 50, 60)            29040

 LSTM2 (LSTM)                (None, 60)                29040

 dropout5 (Dropout)          (None, 60)                0

 Linear (Dense)              (None, 50)                3050

 activation (Activation)     (None, 50)                0

=================================================================
Total params: 224,732
Trainable params: 143,030
Non-trainable params: 81,702
_________________________________________________________________
2023-08-26 13:40:37.829742: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8900
2023-08-26 13:40:38.765443: I tensorflow/stream_executor/cuda/cuda_blas.cc:1774] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
Epoch 1/20
  6/100 [>.............................] - ETA: 5s - loss: 0.1962 - accuracy: 0.9688WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0302s vs `on_train_batch_end` time: 0.0377s). Check your callbacks.
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0302s vs `on_train_batch_end` time: 0.0377s). Check your callbacks.
100/100 [==============================] - 10s 73ms/step - loss: 0.3219 - accuracy: 0.9262 - val_loss: 3.8080 - val_accuracy: 0.1900
Epoch 2/20
100/100 [==============================] - 7s 68ms/step - loss: 0.4272 - accuracy: 0.8863 - val_loss: 2.0887 - val_accuracy: 0.4625
Epoch 3/20
100/100 [==============================] - 7s 68ms/step - loss: 0.3462 - accuracy: 0.9038 - val_loss: 1.8368 - val_accuracy: 0.4925
Epoch 4/20
100/100 [==============================] - 7s 68ms/step - loss: 0.3208 - accuracy: 0.9144 - val_loss: 1.3799 - val_accuracy: 0.5850
Epoch 5/20
100/100 [==============================] - 7s 69ms/step - loss: 0.3305 - accuracy: 0.9137 - val_loss: 1.8779 - val_accuracy: 0.5200
Epoch 6/20
100/100 [==============================] - 7s 68ms/step - loss: 0.3868 - accuracy: 0.8981 - val_loss: 1.6784 - val_accuracy: 0.5475
Epoch 7/20
100/100 [==============================] - 7s 68ms/step - loss: 0.3334 - accuracy: 0.9006 - val_loss: 2.3369 - val_accuracy: 0.4000
Epoch 8/20
100/100 [==============================] - 7s 68ms/step - loss: 0.3611 - accuracy: 0.8956 - val_loss: 1.5716 - val_accuracy: 0.5700
Epoch 9/20
100/100 [==============================] - 7s 68ms/step - loss: 0.2990 - accuracy: 0.9262 - val_loss: 1.7485 - val_accuracy: 0.5375
Epoch 10/20
100/100 [==============================] - 7s 68ms/step - loss: 0.3118 - accuracy: 0.9187 - val_loss: 1.6809 - val_accuracy: 0.5600
Epoch 11/20
100/100 [==============================] - 7s 68ms/step - loss: 0.2834 - accuracy: 0.9175 - val_loss: 2.7306 - val_accuracy: 0.3175
Epoch 12/20
100/100 [==============================] - 7s 68ms/step - loss: 0.3048 - accuracy: 0.9081 - val_loss: 1.5370 - val_accuracy: 0.6100
Epoch 13/20
100/100 [==============================] - 7s 68ms/step - loss: 0.3128 - accuracy: 0.9100 - val_loss: 1.5147 - val_accuracy: 0.5950
Epoch 14/20
100/100 [==============================] - 7s 68ms/step - loss: 0.2754 - accuracy: 0.9306 - val_loss: 1.3564 - val_accuracy: 0.6075
Epoch 15/20
100/100 [==============================] - 7s 68ms/step - loss: 0.2989 - accuracy: 0.9150 - val_loss: 1.5094 - val_accuracy: 0.6050
Epoch 16/20
100/100 [==============================] - 7s 68ms/step - loss: 0.2550 - accuracy: 0.9294 - val_loss: 1.6450 - val_accuracy: 0.5875
Epoch 17/20
100/100 [==============================] - 7s 68ms/step - loss: 0.2508 - accuracy: 0.9337 - val_loss: 1.3674 - val_accuracy: 0.6100
Epoch 18/20
100/100 [==============================] - 7s 69ms/step - loss: 0.2581 - accuracy: 0.9294 - val_loss: 1.3144 - val_accuracy: 0.6300
Epoch 19/20
100/100 [==============================] - 7s 69ms/step - loss: 0.2145 - accuracy: 0.9431 - val_loss: 1.5413 - val_accuracy: 0.5725
Epoch 20/20
100/100 [==============================] - 7s 68ms/step - loss: 0.2221 - accuracy: 0.9388 - val_loss: 1.4340 - val_accuracy: 0.6250
Baseline test loss: 1.0154744386672974
Pruned test loss: 1.434005618095398
Baseline test accuracy: 0.6800000071525574
Pruned test accuracy: 0.625
WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.
WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 conv1 (Conv2D)              (None, 126, 1249, 30)     300

 batch_normalization (BatchN  (None, 126, 1249, 30)    120
 ormalization)

 pool1 (MaxPooling2D)        (None, 42, 624, 30)       0

 dropout1 (Dropout)          (None, 42, 624, 30)       0

 conv2 (Conv2D)              (None, 40, 622, 60)       16260

 batch_normalization_1 (Batc  (None, 40, 622, 60)      240
 hNormalization)

 pool2 (MaxPooling2D)        (None, 20, 311, 60)       0

 dropout2 (Dropout)          (None, 20, 311, 60)       0

 conv3 (Conv2D)              (None, 18, 309, 60)       32460

 batch_normalization_2 (Batc  (None, 18, 309, 60)      240
 hNormalization)

 pool3 (MaxPooling2D)        (None, 6, 154, 60)        0

 dropout3 (Dropout)          (None, 6, 154, 60)        0

 conv4 (Conv2D)              (None, 4, 152, 60)        32460

 batch_normalization_3 (Batc  (None, 4, 152, 60)       240
 hNormalization)

 pool4 (MaxPooling2D)        (None, 1, 50, 60)         0

 dropout4 (Dropout)          (None, 1, 50, 60)         0

 permute (Permute)           (None, 60, 1, 50)         0

 reshape (Reshape)           (None, 50, 60)            0

 LSTM1 (LSTM)                (None, 50, 60)            29040

 LSTM2 (LSTM)                (None, 60)                29040

 dropout5 (Dropout)          (None, 60)                0

 Linear (Dense)              (None, 50)                3050

 activation (Activation)     (None, 50)                0

=================================================================
Total params: 143,450
Trainable params: 143,030
Non-trainable params: 420
_________________________________________________________________
WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.
2023-08-26 13:43:08.344404: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:363] Ignored output_format.
2023-08-26 13:43:08.344486: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:366] Ignored drop_control_dependency.
2023-08-26 13:43:08.344655: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: C:\Users\34405\AppData\Local\Temp\tmp9g_c0utr
2023-08-26 13:43:08.356839: I tensorflow/cc/saved_model/reader.cc:107] Reading meta graph with tags { serve }
2023-08-26 13:43:08.356914: I tensorflow/cc/saved_model/reader.cc:148] Reading SavedModel debug info (if present) from: C:\Users\34405\AppData\Local\Temp\tmp9g_c0utr
2023-08-26 13:43:08.421603: I tensorflow/cc/saved_model/loader.cc:210] Restoring SavedModel bundle.
2023-08-26 13:43:08.517121: I tensorflow/cc/saved_model/loader.cc:194] Running initialization op on SavedModel bundle at path: C:\Users\34405\AppData\Local\Temp\tmp9g_c0utr
2023-08-26 13:43:08.594496: I tensorflow/cc/saved_model/loader.cc:283] SavedModel load for tags { serve }; Status: success: OK. Took 249834 microseconds.
2023-08-26 13:43:08.900184: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1891] TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following Select TFop(s):
Flex ops: FlexTensorListFromTensor, FlexTensorListGetItem, FlexTensorListReserve, FlexTensorListSetItem, FlexTensorListStack
Details:
	tf.TensorListFromTensor(tensor<50x?x60xf32>, tensor<2xi32>) -> (tensor<!tf_type.variant<tensor<?x60xf32>>>) : {device = ""}
	tf.TensorListFromTensor(tensor<?x?x60xf32>, tensor<2xi32>) -> (tensor<!tf_type.variant<tensor<?x60xf32>>>) : {device = ""}
	tf.TensorListGetItem(tensor<!tf_type.variant<tensor<?x60xf32>>>, tensor<i32>, tensor<2xi32>) -> (tensor<?x60xf32>) : {device = ""}
	tf.TensorListReserve(tensor<2xi32>, tensor<i32>) -> (tensor<!tf_type.variant<tensor<?x60xf32>>>) : {device = ""}
	tf.TensorListSetItem(tensor<!tf_type.variant<tensor<?x60xf32>>>, tensor<i32>, tensor<?x60xf32>) -> (tensor<!tf_type.variant<tensor<?x60xf32>>>) : {device = ""}
	tf.TensorListStack(tensor<!tf_type.variant<tensor<?x60xf32>>>, tensor<2xi32>) -> (tensor<?x?x60xf32>) : {device = "", num_elements = -1 : i64}
See instructions: https://www.tensorflow.org/lite/guide/ops_select
Size of gzipped baseline Keras model: 2129538.00 bytes
Size of gzipped baseline TFlite model: 540489.00 bytes
Size of gzipped pruned Keras model: 333681.00 bytes
Size of gzipped pruned TFlite model: 308902.00 bytes

进程已结束,退出代码0
